{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Importing Libraries"],"metadata":{"id":"-blZsc7N6bjf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfH--I6p0gn8"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from folium.plugins import HeatMap\n","\n","from google.colab import drive\n","\n","from pyspark import SparkContext\n","\n","from pyspark.sql import SparkSession, Row\n","from pyspark.sql.functions import col, when, regexp_replace, monotonically_increasing_id, udf, count, sum\n","from pyspark.sql.types import StructType, StructField, DoubleType, StringType, TimestampType, IntegerType\n","\n","from datetime import datetime\n","\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA"]},{"cell_type":"markdown","metadata":{"id":"_QxqrZN2cSQN"},"source":["## Data Uploading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TZf7zdctro_","outputId":"1b453f2d-7d9c-4401-addb-e7ddf0b247b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')\n","\n","path = \"/content/drive/MyDrive/Colab Notebooks/Distributed Data Analysis and Mining/Project/data\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puMMGOw7ExgQ"},"outputs":[],"source":["# Create a SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"LargeRDDToDF\") \\\n","    .master(\"local[*]\") \\\n","    .config(\"spark.executor.memory\", \"8g\") \\\n","    .config(\"spark.driver.memory\", \"8g\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n","    .getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SA6Gn7zoE4R_"},"outputs":[],"source":["df = spark.read.csv(path + \"/df_final_with_weather.csv\", header = True, inferSchema = True, sep = \",\")"]},{"cell_type":"markdown","source":["## Data Manipulation"],"metadata":{"id":"flAw-vFBOv3Y"}},{"cell_type":"code","source":["cols_cluster = [\"Actual_Departure_Time\", \"Departure_Delay_Minutes\", \"Taxi_Out_Time\", \"Taxi_In_Time\", \"Arrival_Delay_Minutes\", \"Flight_Cancelled\", \"Flight_Diverted\",\n","                \"Actual_Flight_Duration\", \"Airborne_Time\", \"Flight_Distance\", \"population_origin_ok\", \"tavg\", \"wspd\", \"wdir\", \"pres\"] # Define columns for clustering\n","\n","df = df.fillna(0, subset = cols_cluster)"],"metadata":{"id":"ryoT9GaV62Ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assembling\n","\n","vec_assembler = VectorAssembler(inputCols = cols_cluster,\n","                                outputCol='features',\n","                                handleInvalid = \"keep\"\n","                                )\n","\n","df = vec_assembler.transform(df)"],"metadata":{"id":"7VT19SWD68gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scaling\n","\n","scaler = StandardScaler(inputCol=\"features\",\n","                        outputCol=\"scaledFeatures\",\n","                        withStd=True,\n","                        withMean=False)\n","\n","scalerModel = scaler.fit(df) # Compute summary statistics by fitting the StandardScaler\n","df = scalerModel.transform(df) # Normalize each feature to have unit standard deviation"],"metadata":{"id":"nxiq-gWn7MAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## KMeans"],"metadata":{"id":"QpgnkbTPOzlg"}},{"cell_type":"code","source":["silhouette_score=[]\n","\n","evaluator = ClusteringEvaluator(predictionCol='prediction',\n","                                featuresCol='scaledFeatures',\n","                                metricName='silhouette',\n","                                distanceMeasure='squaredEuclidean')\n","\n","for i in range(2,10):\n","    kmeans=KMeans(featuresCol='scaledFeatures', k=i)\n","    model=kmeans.fit(df)\n","    predictions=model.transform(df)\n","    score=evaluator.evaluate(predictions)\n","    silhouette_score.append(score)\n","    print('Silhouette Score for k =',i,'is',score)\n","    del model, predictions"],"metadata":{"id":"d89LNIbXKH3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k_values = range(2, 10)\n","\n","plt.plot(k_values, silhouette_score)\n","plt.xlabel('k')\n","plt.ylabel('Silhouette Score')\n","plt.title('Silhouette Score vs. k')\n","plt.show()"],"metadata":{"id":"sgfkzigEKLBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans = KMeans(featuresCol='scaledFeatures', k=3, initMode=\"k-means||\")\n","model = kmeans.fit(df)\n","\n","result  = model.transform(df)\n","result.show(5)"],"metadata":{"id":"mgNVIKcdPM46"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization"],"metadata":{"id":"2__h2CwCPRVO"}},{"cell_type":"code","source":["# Get cluster centroids\n","\n","centroids = model.clusterCenters()\n","centroids_df = pd.DataFrame(centroids, columns=cols_cluster)\n","centroids_df['cluster'] = centroids_df.index\n","\n","centroids_df"],"metadata":{"id":"UKahl7OjPQZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PCA"],"metadata":{"id":"BoxM48ioD60K"}},{"cell_type":"code","source":["spark_df = spark.createDataFrame(centroids_df) # Convert df to spark df\n","\n","vec_assembler = VectorAssembler(inputCols=cols_cluster[:-1], outputCol=\"features\") # Vectorize features\n","spark_df = vec_assembler.transform(spark_df)"],"metadata":{"id":"RugSzFFLPeYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply PCA\n","\n","pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n","model = pca.fit(spark_df)\n","rs = model.transform(spark_df)"],"metadata":{"id":"k-2wFMxsEBqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca_df = rs.select(\"pcaFeatures\", \"cluster\").toPandas()\n","pca_df['x'] = pca_df['pcaFeatures'].apply(lambda x: x[0])\n","pca_df['y'] = pca_df['pcaFeatures'].apply(lambda x: x[1])\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(pca_df['x'], pca_df['y'], c=pca_df['cluster'], cmap='viridis', alpha=0.7, s = 1000)\n","\n","plt.title(\"Centroids PCA\")\n","plt.xlabel(\"PC1\")\n","plt.ylabel(\"PC2\")\n","plt.colorbar(label='Cluster')\n","plt.show()"],"metadata":{"id":"Gi0PHiihEDTL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Results"],"metadata":{"id":"LEMUtWYeQG-1"}},{"cell_type":"markdown","source":["### Analytical Results"],"metadata":{"id":"8jHJSUUwEODn"}},{"cell_type":"code","source":["cluster_stats = result.groupBy(\"prediction\") \\\n","    .agg(\n","        (sum(when(col(\"Flight_Cancelled\") == 1, 1)).alias(\"Cancelled_Flights\")),\n","        (count(\"*\")).alias(\"Total_Flights\"),\n","        (sum(when(col(\"Flight_Diverted\") == 1, 1)).alias(\"Diverted_Flights\"))\n","    ) \\\n","    .withColumn(\"Cancelled_Percentage\", (col(\"Cancelled_Flights\") / col(\"Total_Flights\")) * 100) \\\n","    .withColumn(\"Diverted_Percentage\", (col(\"Diverted_Flights\") / col(\"Total_Flights\")) * 100) # Calculate the percentage of Flight_cancelled = 1 and Flight_diverted = 1 for each cluster\n","\n","cluster_stats.show()"],"metadata":{"id":"Pp0lFDAlQKQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by airline and cluster to see the frequency of each airline in each cluster\n","\n","airline_cluster_freq = result.groupBy(\"Operating_Carrier\", \"prediction\").count()\n","\n","acf = airline_cluster_freq.toPandas()\n","\n","top_airlines_per_cluster = acf.groupby('prediction').apply(lambda x: x.nlargest(2, 'count')) # Group by cluster and airline, then sort by count within each cluster\n","\n","top_airlines_per_cluster"],"metadata":{"id":"UKOcpLp2QUET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Geographical Analysis"],"metadata":{"id":"NZu6qB9qQpcD"}},{"cell_type":"code","source":["# Group by origin and destination data\n","\n","grouped_df = result.groupBy(\"Origin_Airport\", \"latitude\", \"longitude\", \"Destination_Airport\", \"latitude_dest\", \"longitude_dest\", \"prediction\").count()\n","\n","sorted_df = airport_df.groupby(['Origin_Airport', 'latitude', 'longitude', 'prediction'], as_index=False).agg({\n","    'count': 'sum'}) # Group by only origin to get unique airports\n","sorted_df = sorted_df.sort_values(by=['Origin_Airport', 'count'], ascending=[True, False])\n","sorted_df = sorted_df.drop_duplicates(subset=['Origin_Airport'], keep='first') # Drop duplicates"],"metadata":{"id":"4oRQib8iQr-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aggregating and separating data into clusters\n","\n","sorted_df = airport_df.groupby(['Origin_Airport', 'latitude', 'longitude', 'prediction'], as_index=False).agg({\n","    'count': 'sum'})\n","\n","repr_airports0 = (\n","    airport_df[airport_df['prediction']==0].groupby('prediction', group_keys=False)\n","    .apply(lambda x: x.nlargest(500, 'count'))\n",")\n","\n","repr_airports1 = (\n","    airport_df[airport_df['prediction']==1].groupby('prediction', group_keys=False)\n","    .apply(lambda x: x.nlargest(500, 'count'))\n",")\n","\n","repr_airports2 = (\n","    airport_df[airport_df['prediction']==2].groupby('prediction', group_keys=False)\n","    .apply(lambda x: x.nlargest(500, 'count'))\n",")"],"metadata":{"id":"ZtZXF78vRYX-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Top Airports per Cluster"],"metadata":{"id":"V9Fb4e4WTE72"}},{"cell_type":"code","source":["m = folium.Map(location=[39.8283, -98.5795], zoom_start=4)\n","\n","airport_coords = repr_airports2[['latitude', 'longitude']].values.tolist()\n","\n","HeatMap(airport_coords).add_to(m) # Add data points to map"],"metadata":{"id":"EYBieTgHScaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m"],"metadata":{"id":"to5z6lZOE0IH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Top Routes per Cluster"],"metadata":{"id":"80ipv60rTIJA"}},{"cell_type":"code","source":["m = folium.Map(location=[39.8283, -98.5795], zoom_start=4)\n","\n","cluster_colors = ['blue', 'red', 'green']\n","\n","for _, row in top_airports_by_prediction.iterrows():\n","    origin = (row['latitude'], row['longitude'])\n","    destination = (row['latitude_dest'], row['longitude_dest'])\n","    frequency = row['count']\n","    cluster_origin = int(row['prediction'])\n","    color_origin = cluster_colors[cluster_origin % len(cluster_colors)]\n","\n","    folium.PolyLine( # Plot a line according to the frequency\n","        locations=[origin, destination],\n","        color='blue',\n","        weight = 2.5 + 5 * (np.log(frequency) - np.log(min_counts)) / (np.log(max_counts) - np.log(min_counts)),\n","        opacity=0.7\n","    ).add_to(m) # Iterate over most frequent routes\n","\n","    folium.Marker(location=origin, popup=\"Origin\", icon=folium.Icon(color=color_origin)).add_to(m)\n","    folium.Marker(location=destination, popup=\"Destination\", icon=folium.Icon(color=color_origin)).add_to(m)"],"metadata":{"id":"vgNLB9olTDF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m"],"metadata":{"id":"X0gAFp1nFFga"},"execution_count":null,"outputs":[]}]}